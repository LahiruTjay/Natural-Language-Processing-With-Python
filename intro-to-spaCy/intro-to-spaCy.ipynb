{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK vs spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Tokenization (Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"data/sample_text.txt\", \"r\", encoding=\"utf8\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.vocab.Vocab object at 0x000000DE741D83C8>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(text)\n",
    "print(doc.vocab)\n",
    "#for token in doc:\n",
    "#    print('\"' + token.text + '\"', token.idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The\t0\t﻿the\tFalse\tFalse\t﻿Xxx\tDET\tDT\n",
      "performance\t5\tperformance\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t17\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "these\t20\tthese\tFalse\tFalse\txxxx\tDET\tDT\n",
      "simple\t26\tsimple\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "machine\t33\tmachine\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "learning\t41\tlearn\tFalse\tFalse\txxxx\tVERB\tVBG\n",
      "algorithms\t50\talgorithm\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "depends\t61\tdepend\tFalse\tFalse\txxxx\tVERB\tVBZ\n",
      "heavily\t69\theavily\tFalse\tFalse\txxxx\tADV\tRB\n",
      "on\t77\ton\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t80\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "representation\t84\trepresentation\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t99\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t102\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "data\t106\tdatum\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "they\t111\t-PRON-\tFalse\tFalse\txxxx\tPRON\tPRP\n",
      "are\t116\tbe\tFalse\tFalse\txxx\tVERB\tVBP\n",
      "given\t120\tgive\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      ".\t125\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "For\t127\tfor\tFalse\tFalse\tXxx\tADP\tIN\n",
      "example\t131\texample\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t138\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "when\t140\twhen\tFalse\tFalse\txxxx\tADV\tWRB\n",
      "logistic\t145\tlogistic\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "regression\t154\tregression\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "is\t165\tbe\tFalse\tFalse\txx\tVERB\tVBZ\n",
      "used\t168\tuse\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "to\t173\tto\tFalse\tFalse\txx\tPART\tTO\n",
      "recommend\t176\trecommend\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "cesarean\t186\tcesarean\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "delivery\t195\tdelivery\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t203\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "the\t205\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "AI\t209\tai\tFalse\tFalse\tXX\tPROPN\tNNP\n",
      "system\t212\tsystem\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "does\t219\tdo\tFalse\tFalse\txxxx\tVERB\tVBZ\n",
      "not\t224\tnot\tFalse\tFalse\txxx\tADV\tRB\n",
      "examinethe\t228\texaminethe\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "patient\t239\tpatient\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "directly\t247\tdirectly\tFalse\tFalse\txxxx\tADV\tRB\n",
      ".\t255\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "Instead\t257\tinstead\tFalse\tFalse\tXxxxx\tADV\tRB\n",
      ",\t264\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "the\t266\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "doctor\t270\tdoctor\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "tells\t277\ttell\tFalse\tFalse\txxxx\tVERB\tVBZ\n",
      "the\t283\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "system\t287\tsystem\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "several\t294\tseveral\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "pieces\t302\tpiece\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "of\t309\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "relevant\t312\trelevant\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "information\t321\tinformation\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t332\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "such\t334\tsuch\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "as\t339\tas\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t342\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "presence\t346\tpresence\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "or\t355\tor\tFalse\tFalse\txx\tCCONJ\tCC\n",
      "absence\t358\tabsence\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t366\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "a\t369\ta\tFalse\tFalse\tx\tDET\tDT\n",
      "uterine\t371\tuterine\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "scar\t379\tscar\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ".\t383\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "Each\t385\teach\tFalse\tFalse\tXxxx\tDET\tDT\n",
      "piece\t390\tpiece\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t396\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "information\t399\tinformation\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "included\t411\tinclude\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "in\t420\tin\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t423\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "representation\t427\trepresentation\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t442\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t445\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "patient\t449\tpatient\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "is\t457\tbe\tFalse\tFalse\txx\tVERB\tVBZ\n",
      "known\t460\tknow\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "as\t466\tas\tFalse\tFalse\txx\tADP\tIN\n",
      "afeature\t469\tafeature\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ".\t477\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "Logistic\t479\tlogistic\tFalse\tFalse\tXxxxx\tADJ\tJJ\n",
      "regression\t488\tregression\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "learns\t499\tlearn\tFalse\tFalse\txxxx\tVERB\tVBZ\n",
      "how\t506\thow\tFalse\tFalse\txxx\tADV\tWRB\n",
      "each\t510\teach\tFalse\tFalse\txxxx\tDET\tDT\n",
      "of\t515\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "these\t518\tthese\tFalse\tFalse\txxxx\tDET\tDT\n",
      "features\t524\tfeature\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "of\t533\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t536\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "patient\t540\tpatient\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "correlates\t548\tcorrelate\tFalse\tFalse\txxxx\tVERB\tVBZ\n",
      "with\t559\twith\tFalse\tFalse\txxxx\tADP\tIN\n",
      "various\t564\tvarious\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "outcomes\t572\toutcome\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      ".\t580\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "However\t582\thowever\tFalse\tFalse\tXxxxx\tADV\tRB\n",
      ",\t589\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "it\t591\t-PRON-\tFalse\tFalse\txx\tPRON\tPRP\n",
      "can\t594\tcan\tFalse\tFalse\txxx\tVERB\tMD\n",
      "not\t597\tnot\tFalse\tFalse\txxx\tADV\tRB\n",
      "inﬂuence\t601\tinﬂuence\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "how\t610\thow\tFalse\tFalse\txxx\tADV\tWRB\n",
      "features\t614\tfeature\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "are\t623\tbe\tFalse\tFalse\txxx\tVERB\tVBP\n",
      "deﬁned\t627\tdeﬁn\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "in\t634\tin\tFalse\tFalse\txx\tADP\tIN\n",
      "anyway\t637\tanyway\tFalse\tFalse\txxxx\tADV\tRB\n",
      ".\t643\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "If\t645\tif\tFalse\tFalse\tXx\tADP\tIN\n",
      "logistic\t648\tlogistic\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "regression\t657\tregression\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "were\t668\tbe\tFalse\tFalse\txxxx\tVERB\tVBD\n",
      "given\t673\tgive\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "an\t679\tan\tFalse\tFalse\txx\tDET\tDT\n",
      "MRI\t682\tmri\tFalse\tFalse\tXXX\tPROPN\tNNP\n",
      "scan\t686\tscan\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t691\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t694\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "patient\t698\tpatient\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t705\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "rather\t707\trather\tFalse\tFalse\txxxx\tADV\tRB\n",
      "thanthe\t714\tthanthe\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "doctor\t722\tdoctor\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "’s\t728\t’s\tFalse\tFalse\t’x\tPART\tPOS\n",
      "formalized\t731\tformalize\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "report\t742\treport\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t748\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "it\t750\t-PRON-\tFalse\tFalse\txx\tPRON\tPRP\n",
      "would\t753\twould\tFalse\tFalse\txxxx\tVERB\tMD\n",
      "not\t759\tnot\tFalse\tFalse\txxx\tADV\tRB\n",
      "be\t763\tbe\tFalse\tFalse\txx\tVERB\tVB\n",
      "able\t766\table\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "to\t771\tto\tFalse\tFalse\txx\tPART\tTO\n",
      "make\t774\tmake\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "useful\t779\tuseful\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "predictions\t786\tprediction\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      ".\t797\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "Individual\t799\tindividual\tFalse\tFalse\tXxxxx\tADJ\tJJ\n",
      "pixels\t810\tpixel\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "in\t817\tin\tFalse\tFalse\txx\tADP\tIN\n",
      "an\t820\tan\tFalse\tFalse\txx\tDET\tDT\n",
      "MRI\t823\tmri\tFalse\tFalse\tXXX\tPROPN\tNNP\n",
      "scan\t827\tscan\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "have\t832\thave\tFalse\tFalse\txxxx\tVERB\tVBP\n",
      "negligible\t837\tnegligible\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "correlation\t848\tcorrelation\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "with\t860\twith\tFalse\tFalse\txxxx\tADP\tIN\n",
      "any\t865\tany\tFalse\tFalse\txxx\tDET\tDT\n",
      "complications\t869\tcomplication\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "that\t883\tthat\tFalse\tFalse\txxxx\tADJ\tWDT\n",
      "might\t888\tmay\tFalse\tFalse\txxxx\tVERB\tMD\n",
      "occur\t894\toccur\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "during\t900\tduring\tFalse\tFalse\txxxx\tADP\tIN\n",
      "delivery\t907\tdelivery\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ".\t915\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "This\t916\tthis\tFalse\tFalse\tXxxx\tDET\tDT\n",
      "dependence\t921\tdependence\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "on\t932\ton\tFalse\tFalse\txx\tADP\tIN\n",
      "representations\t935\trepresentation\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "is\t951\tbe\tFalse\tFalse\txx\tVERB\tVBZ\n",
      "a\t954\ta\tFalse\tFalse\tx\tDET\tDT\n",
      "general\t956\tgeneral\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "phenomenon\t964\tphenomenon\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "that\t975\tthat\tFalse\tFalse\txxxx\tADJ\tWDT\n",
      "appears\t980\tappear\tFalse\tFalse\txxxx\tVERB\tVBZ\n",
      "throughout\t988\tthroughout\tFalse\tFalse\txxxx\tADP\tIN\n",
      "computer\t999\tcomputer\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "science\t1008\tscience\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "and\t1016\tand\tFalse\tFalse\txxx\tCCONJ\tCC\n",
      "even\t1020\teven\tFalse\tFalse\txxxx\tADV\tRB\n",
      "daily\t1025\tdaily\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "life\t1031\tlife\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ".\t1035\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "In\t1037\tin\tFalse\tFalse\tXx\tADP\tIN\n",
      "computer\t1040\tcomputer\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "science\t1049\tscience\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t1056\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "operations\t1058\toperation\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "such\t1069\tsuch\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "as\t1074\tas\tFalse\tFalse\txx\tADP\tIN\n",
      "searching\t1077\tsearch\tFalse\tFalse\txxxx\tVERB\tVBG\n",
      "a\t1087\ta\tFalse\tFalse\tx\tDET\tDT\n",
      "collection\t1089\tcollection\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t1100\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "data\t1103\tdatum\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "can\t1108\tcan\tFalse\tFalse\txxx\tVERB\tMD\n",
      "proceed\t1112\tproceed\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "exponentially\t1120\texponentially\tFalse\tFalse\txxxx\tADV\tRB\n",
      "faster\t1134\tfaster\tFalse\tFalse\txxxx\tADV\tRBR\n",
      "if\t1141\tif\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t1144\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "collection\t1148\tcollection\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "is\t1159\tbe\tFalse\tFalse\txx\tVERB\tVBZ\n",
      "structured\t1162\tstructure\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "and\t1173\tand\tFalse\tFalse\txxx\tCCONJ\tCC\n",
      "indexed\t1177\tindex\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "intelligently\t1185\tintelligently\tFalse\tFalse\txxxx\tADV\tRB\n",
      ".\t1198\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "People\t1200\tpeople\tFalse\tFalse\tXxxxx\tNOUN\tNNS\n",
      "can\t1207\tcan\tFalse\tFalse\txxx\tVERB\tMD\n",
      "easily\t1211\teasily\tFalse\tFalse\txxxx\tADV\tRB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform\t1218\tperform\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "arithmetic\t1226\tarithmetic\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "on\t1237\ton\tFalse\tFalse\txx\tADP\tIN\n",
      "Arabic\t1240\tarabic\tFalse\tFalse\tXxxxx\tADJ\tJJ\n",
      "numerals\t1247\tnumeral\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "but\t1256\tbut\tFalse\tFalse\txxx\tCCONJ\tCC\n",
      "ﬁnd\t1260\tﬁnd\tFalse\tFalse\txxx\tNOUN\tNN\n",
      "arithmetic\t1264\tarithmetic\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "on\t1275\ton\tFalse\tFalse\txx\tADP\tIN\n",
      "Roman\t1278\troman\tFalse\tFalse\tXxxxx\tADJ\tJJ\n",
      "numerals\t1284\tnumeral\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "much\t1293\tmuch\tFalse\tFalse\txxxx\tADV\tRB\n",
      "more\t1298\tmore\tFalse\tFalse\txxxx\tADJ\tJJR\n",
      "time\t1303\ttime\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "consuming\t1308\tconsume\tFalse\tFalse\txxxx\tVERB\tVBG\n",
      ".\t1317\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "It\t1319\t-PRON-\tFalse\tFalse\tXx\tPRON\tPRP\n",
      "is\t1322\tbe\tFalse\tFalse\txx\tVERB\tVBZ\n",
      "not\t1325\tnot\tFalse\tFalse\txxx\tADV\tRB\n",
      "surprising\t1329\tsurprising\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "that\t1340\tthat\tFalse\tFalse\txxxx\tADP\tIN\n",
      "the\t1345\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "choice\t1349\tchoice\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t1356\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "representation\t1359\trepresentation\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "has\t1374\thave\tFalse\tFalse\txxx\tVERB\tVBZ\n",
      "an\t1378\tan\tFalse\tFalse\txx\tDET\tDT\n",
      "enormous\t1381\tenormous\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "eﬀect\t1390\teﬀect\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "on\t1396\ton\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t1399\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "performance\t1403\tperformance\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t1415\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "machine\t1418\tmachine\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "learning\t1426\tlearn\tFalse\tFalse\txxxx\tVERB\tVBG\n",
      "algorithms\t1435\talgorithm\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      ".\t1445\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "For\t1447\tfor\tFalse\tFalse\tXxx\tADP\tIN\n",
      "a\t1451\ta\tFalse\tFalse\tx\tDET\tDT\n",
      "simple\t1453\tsimple\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "visual\t1460\tvisual\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "example\t1467\texample\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t1474\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "see\t1476\tsee\tFalse\tFalse\txxx\tVERB\tVBP\n",
      "ﬁgure\t1480\tﬁgure\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "1.1\t1486\t1.1\tFalse\tFalse\td.d\tNUM\tCD\n",
      ".\t1489\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "Many\t1491\tmany\tFalse\tFalse\tXxxx\tADJ\tJJ\n",
      "artiﬁcial\t1496\tartiﬁcial\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "intelligence\t1506\tintelligence\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "tasks\t1519\ttask\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "can\t1525\tcan\tFalse\tFalse\txxx\tVERB\tMD\n",
      "be\t1529\tbe\tFalse\tFalse\txx\tVERB\tVB\n",
      "solved\t1532\tsolve\tFalse\tFalse\txxxx\tVERB\tVBN\n",
      "by\t1539\tby\tFalse\tFalse\txx\tADP\tIN\n",
      "designing\t1542\tdesign\tFalse\tFalse\txxxx\tVERB\tVBG\n",
      "the\t1552\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "right\t1556\tright\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "set\t1562\tset\tFalse\tFalse\txxx\tNOUN\tNN\n",
      "offeatures\t1566\toffeature\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "to\t1577\tto\tFalse\tFalse\txx\tPART\tTO\n",
      "extract\t1580\textract\tFalse\tFalse\txxxx\tVERB\tVB\n",
      "for\t1588\tfor\tFalse\tFalse\txxx\tADP\tIN\n",
      "that\t1592\tthat\tFalse\tFalse\txxxx\tDET\tDT\n",
      "task\t1597\ttask\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t1601\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "then\t1603\tthen\tFalse\tFalse\txxxx\tADV\tRB\n",
      "providing\t1608\tprovide\tFalse\tFalse\txxxx\tVERB\tVBG\n",
      "these\t1618\tthese\tFalse\tFalse\txxxx\tDET\tDT\n",
      "features\t1624\tfeature\tFalse\tFalse\txxxx\tNOUN\tNNS\n",
      "to\t1633\tto\tFalse\tFalse\txx\tADP\tIN\n",
      "a\t1636\ta\tFalse\tFalse\tx\tDET\tDT\n",
      "simple\t1638\tsimple\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "machine\t1645\tmachine\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "learning\t1653\tlearn\tFalse\tFalse\txxxx\tVERB\tVBG\n",
      "algorithm\t1662\talgorithm\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ".\t1671\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "For\t1673\tfor\tFalse\tFalse\tXxx\tADP\tIN\n",
      "example\t1677\texample\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t1684\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "a\t1686\ta\tFalse\tFalse\tx\tDET\tDT\n",
      "useful\t1688\tuseful\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "feature\t1695\tfeature\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "for\t1703\tfor\tFalse\tFalse\txxx\tADP\tIN\n",
      "speaker\t1707\tspeaker\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "identiﬁcation\t1715\tidentiﬁcation\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "from\t1729\tfrom\tFalse\tFalse\txxxx\tADP\tIN\n",
      "sound\t1734\tsound\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "is\t1740\tbe\tFalse\tFalse\txx\tVERB\tVBZ\n",
      "an\t1743\tan\tFalse\tFalse\txx\tDET\tDT\n",
      "estimate\t1746\testimate\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t1755\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t1758\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "size\t1762\tsize\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "of\t1767\tof\tFalse\tFalse\txx\tADP\tIN\n",
      "the\t1770\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "speaker\t1774\tspeaker\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "’s\t1781\t’s\tFalse\tFalse\t’x\tPART\tPOS\n",
      "vocal\t1784\tvocal\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "tract\t1790\ttract\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ".\t1795\t.\tTrue\tFalse\t.\tPUNCT\t.\n",
      "This\t1797\tthis\tFalse\tFalse\tXxxx\tDET\tDT\n",
      "feature\t1802\tfeature\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "gives\t1810\tgive\tFalse\tFalse\txxxx\tVERB\tVBZ\n",
      "astrong\t1816\tastrong\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "clue\t1824\tclue\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "as\t1829\tas\tFalse\tFalse\txx\tADP\tIN\n",
      "to\t1832\tto\tFalse\tFalse\txx\tADP\tIN\n",
      "whether\t1835\twhether\tFalse\tFalse\txxxx\tADP\tIN\n",
      "the\t1843\tthe\tFalse\tFalse\txxx\tDET\tDT\n",
      "speaker\t1847\tspeaker\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "is\t1855\tbe\tFalse\tFalse\txx\tVERB\tVBZ\n",
      "a\t1858\ta\tFalse\tFalse\tx\tDET\tDT\n",
      "man\t1860\tman\tFalse\tFalse\txxx\tNOUN\tNN\n",
      ",\t1863\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "woman\t1865\twoman\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ",\t1870\t,\tTrue\tFalse\t,\tPUNCT\t,\n",
      "or\t1872\tor\tFalse\tFalse\txx\tCCONJ\tCC\n",
      "child\t1875\tchild\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      ".\t1880\t.\tTrue\tFalse\t.\tPUNCT\t.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25614997799799266 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "sent_tokens = sent_tokenize(text)\n",
    "sentences = [sent for sent in sent_tokens]\n",
    "\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010700373713395828 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "doc = nlp(text)\n",
    "sentences = [sent for sent in doc.sents]\n",
    "\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Tokenization (Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010634089726281326 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "words = [word_token for word_token in word_tokens]\n",
    "\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009429859353932035 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "doc = nlp(text)\n",
    "words = [token.text for token in doc]\n",
    "\n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34786349603985256 seconds\n",
      "[('\\ufeffThe', 'JJ'), ('performance', 'NN'), ('of', 'IN'), ('these', 'DT'), ('simple', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'JJ'), ('depends', 'NNS'), ('heavily', 'RB'), ('on', 'IN'), ('the', 'DT'), ('representation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('data', 'NN'), ('they', 'PRP'), ('are', 'VBP'), ('given', 'VBN'), ('.', '.'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('when', 'WRB'), ('logistic', 'JJ'), ('regression', 'NN'), ('is', 'VBZ'), ('used', 'VBN'), ('to', 'TO'), ('recommend', 'VB'), ('cesarean', 'JJ'), ('delivery', 'NN'), (',', ','), ('the', 'DT'), ('AI', 'NNP'), ('system', 'NN'), ('does', 'VBZ'), ('not', 'RB'), ('examinethe', 'VB'), ('patient', 'NN'), ('directly', 'RB'), ('.', '.'), ('Instead', 'RB'), (',', ','), ('the', 'DT'), ('doctor', 'NN'), ('tells', 'VBZ'), ('the', 'DT'), ('system', 'NN'), ('several', 'JJ'), ('pieces', 'NNS'), ('of', 'IN'), ('relevant', 'JJ'), ('information', 'NN'), (',', ','), ('such', 'JJ'), ('as', 'IN'), ('the', 'DT'), ('presence', 'NN'), ('or', 'CC'), ('absence', 'NN'), ('of', 'IN'), ('a', 'DT'), ('uterine', 'JJ'), ('scar', 'NN'), ('.', '.'), ('Each', 'DT'), ('piece', 'NN'), ('of', 'IN'), ('information', 'NN'), ('included', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('representation', 'NN'), ('of', 'IN'), ('the', 'DT'), ('patient', 'NN'), ('is', 'VBZ'), ('known', 'VBN'), ('as', 'IN'), ('afeature', 'NN'), ('.', '.'), ('Logistic', 'JJ'), ('regression', 'NN'), ('learns', 'VBZ'), ('how', 'WRB'), ('each', 'DT'), ('of', 'IN'), ('these', 'DT'), ('features', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('patient', 'NN'), ('correlates', 'VBZ'), ('with', 'IN'), ('various', 'JJ'), ('outcomes', 'NNS'), ('.', '.'), ('However', 'RB'), (',', ','), ('it', 'PRP'), ('can', 'MD'), ('not', 'RB'), ('inﬂuence', 'VB'), ('how', 'WRB'), ('features', 'NNS'), ('are', 'VBP'), ('deﬁned', 'VBN'), ('in', 'IN'), ('anyway', 'RB'), ('.', '.'), ('If', 'IN'), ('logistic', 'JJ'), ('regression', 'NN'), ('were', 'VBD'), ('given', 'VBN'), ('an', 'DT'), ('MRI', 'NNP'), ('scan', 'NN'), ('of', 'IN'), ('the', 'DT'), ('patient', 'NN'), (',', ','), ('rather', 'RB'), ('thanthe', 'JJ'), ('doctor', 'NN'), ('’', 'NNP'), ('s', 'VBZ'), ('formalized', 'VBN'), ('report', 'NN'), (',', ','), ('it', 'PRP'), ('would', 'MD'), ('not', 'RB'), ('be', 'VB'), ('able', 'JJ'), ('to', 'TO'), ('make', 'VB'), ('useful', 'JJ'), ('predictions', 'NNS'), ('.', '.'), ('Individual', 'JJ'), ('pixels', 'NNS'), ('in', 'IN'), ('an', 'DT'), ('MRI', 'NNP'), ('scan', 'NN'), ('have', 'VBP'), ('negligible', 'JJ'), ('correlation', 'NN'), ('with', 'IN'), ('any', 'DT'), ('complications', 'NNS'), ('that', 'WDT'), ('might', 'MD'), ('occur', 'VB'), ('during', 'IN'), ('delivery.This', 'JJ'), ('dependence', 'NN'), ('on', 'IN'), ('representations', 'NNS'), ('is', 'VBZ'), ('a', 'DT'), ('general', 'JJ'), ('phenomenon', 'NN'), ('that', 'IN'), ('appears', 'VBZ'), ('throughout', 'IN'), ('computer', 'NN'), ('science', 'NN'), ('and', 'CC'), ('even', 'RB'), ('daily', 'JJ'), ('life', 'NN'), ('.', '.'), ('In', 'IN'), ('computer', 'NN'), ('science', 'NN'), (',', ','), ('operations', 'NNS'), ('such', 'JJ'), ('as', 'IN'), ('searching', 'VBG'), ('a', 'DT'), ('collection', 'NN'), ('of', 'IN'), ('data', 'NNS'), ('can', 'MD'), ('proceed', 'VB'), ('exponentially', 'RB'), ('faster', 'RBR'), ('if', 'IN'), ('the', 'DT'), ('collection', 'NN'), ('is', 'VBZ'), ('structured', 'VBN'), ('and', 'CC'), ('indexed', 'VBN'), ('intelligently', 'RB'), ('.', '.'), ('People', 'NNS'), ('can', 'MD'), ('easily', 'RB'), ('perform', 'VB'), ('arithmetic', 'JJ'), ('on', 'IN'), ('Arabic', 'NNP'), ('numerals', 'NNS'), ('but', 'CC'), ('ﬁnd', 'NNP'), ('arithmetic', 'JJ'), ('on', 'IN'), ('Roman', 'NNP'), ('numerals', 'NNS'), ('much', 'RB'), ('more', 'JJR'), ('time', 'NN'), ('consuming', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('not', 'RB'), ('surprising', 'JJ'), ('that', 'IN'), ('the', 'DT'), ('choice', 'NN'), ('of', 'IN'), ('representation', 'NN'), ('has', 'VBZ'), ('an', 'DT'), ('enormous', 'JJ'), ('eﬀect', 'NN'), ('on', 'IN'), ('the', 'DT'), ('performance', 'NN'), ('of', 'IN'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithms', 'NNS'), ('.', '.'), ('For', 'IN'), ('a', 'DT'), ('simple', 'JJ'), ('visual', 'JJ'), ('example', 'NN'), (',', ','), ('see', 'VBP'), ('ﬁgure', 'JJ'), ('1.1', 'CD'), ('.', '.'), ('Many', 'JJ'), ('artiﬁcial', 'JJ'), ('intelligence', 'NN'), ('tasks', 'NNS'), ('can', 'MD'), ('be', 'VB'), ('solved', 'VBN'), ('by', 'IN'), ('designing', 'VBG'), ('the', 'DT'), ('right', 'NN'), ('set', 'NN'), ('offeatures', 'NNS'), ('to', 'TO'), ('extract', 'VB'), ('for', 'IN'), ('that', 'DT'), ('task', 'NN'), (',', ','), ('then', 'RB'), ('providing', 'VBG'), ('these', 'DT'), ('features', 'NNS'), ('to', 'TO'), ('a', 'DT'), ('simple', 'JJ'), ('machine', 'NN'), ('learning', 'VBG'), ('algorithm', 'NN'), ('.', '.'), ('For', 'IN'), ('example', 'NN'), (',', ','), ('a', 'DT'), ('useful', 'JJ'), ('feature', 'NN'), ('for', 'IN'), ('speaker', 'NN'), ('identiﬁcation', 'NN'), ('from', 'IN'), ('sound', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('estimate', 'NN'), ('of', 'IN'), ('the', 'DT'), ('size', 'NN'), ('of', 'IN'), ('the', 'DT'), ('speaker', 'NN'), ('’', 'NNP'), ('s', 'RB'), ('vocal', 'JJ'), ('tract', 'NN'), ('.', '.'), ('This', 'DT'), ('feature', 'NN'), ('gives', 'VBZ'), ('astrong', 'RB'), ('clue', 'JJ'), ('as', 'IN'), ('to', 'TO'), ('whether', 'IN'), ('the', 'DT'), ('speaker', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('man', 'NN'), (',', ','), ('woman', 'NN'), (',', ','), ('or', 'CC'), ('child', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "pos_tag_sent = pos_tag(word_tokens)\n",
    "\n",
    "print(time.clock() - start_time, \"seconds\")\n",
    "print(pos_tag_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['parser', 'tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The ﻿The    ﻿Xxx False False\n",
      "performance performance    xxxx True False\n",
      "of of    xx True True\n",
      "these this    xxxx True True\n",
      "simple simple    xxxx True False\n",
      "machine machine    xxxx True False\n",
      "learning learn    xxxx True False\n",
      "algorithms algorithm    xxxx True False\n",
      "depends depend    xxxx True False\n",
      "heavily heavily    xxxx True False\n",
      "on on    xx True True\n",
      "the the    xxx True True\n",
      "representation representation    xxxx True False\n",
      "of of    xx True True\n",
      "the the    xxx True True\n",
      "data datum    xxxx True False\n",
      "they they    xxxx True True\n",
      "are be    xxx True True\n",
      "given give    xxxx True False\n",
      ". .    . False False\n",
      "For For    Xxx True False\n",
      "example example    xxxx True False\n",
      ", ,    , False False\n",
      "when when    xxxx True True\n",
      "logistic logistic    xxxx True False\n",
      "regression regression    xxxx True False\n",
      "is be    xx True True\n",
      "used use    xxxx True True\n",
      "to to    xx True True\n",
      "recommend recommend    xxxx True False\n",
      "cesarean cesarean    xxxx True False\n",
      "delivery delivery    xxxx True False\n",
      ", ,    , False False\n",
      "the the    xxx True True\n",
      "AI AI    XX True False\n",
      "system system    xxxx True False\n",
      "does doe    xxxx True True\n",
      "not not    xxx True True\n",
      "examinethe examinethe    xxxx True False\n",
      "patient patient    xxxx True False\n",
      "directly directly    xxxx True False\n",
      ". .    . False False\n",
      "Instead Instead    Xxxxx True False\n",
      ", ,    , False False\n",
      "the the    xxx True True\n",
      "doctor doctor    xxxx True False\n",
      "tells tell    xxxx True False\n",
      "the the    xxx True True\n",
      "system system    xxxx True False\n",
      "several several    xxxx True True\n",
      "pieces piece    xxxx True False\n",
      "of of    xx True True\n",
      "relevant relevant    xxxx True False\n",
      "information information    xxxx True False\n",
      ", ,    , False False\n",
      "such such    xxxx True True\n",
      "as a    xx True True\n",
      "the the    xxx True True\n",
      "presence presence    xxxx True False\n",
      "or or    xx True True\n",
      "absence absence    xxxx True False\n",
      "of of    xx True True\n",
      "a a    x True True\n",
      "uterine uterine    xxxx True False\n",
      "scar scar    xxxx True False\n",
      ". .    . False False\n",
      "Each Each    Xxxx True False\n",
      "piece piece    xxxx True False\n",
      "of of    xx True True\n",
      "information information    xxxx True False\n",
      "included include    xxxx True False\n",
      "in in    xx True True\n",
      "the the    xxx True True\n",
      "representation representation    xxxx True False\n",
      "of of    xx True True\n",
      "the the    xxx True True\n",
      "patient patient    xxxx True False\n",
      "is be    xx True True\n",
      "known know    xxxx True False\n",
      "as a    xx True True\n",
      "afeature afeature    xxxx True False\n",
      ". .    . False False\n",
      "Logistic Logistic    Xxxxx True False\n",
      "regression regression    xxxx True False\n",
      "learns learn    xxxx True False\n",
      "how how    xxx True True\n",
      "each each    xxxx True True\n",
      "of of    xx True True\n",
      "these this    xxxx True True\n",
      "features feature    xxxx True False\n",
      "of of    xx True True\n",
      "the the    xxx True True\n",
      "patient patient    xxxx True False\n",
      "correlates correlate    xxxx True False\n",
      "with with    xxxx True True\n",
      "various various    xxxx True True\n",
      "outcomes outcome    xxxx True False\n",
      ". .    . False False\n",
      "However However    Xxxxx True False\n",
      ", ,    , False False\n",
      "it it    xx True True\n",
      "can can VERB MD  xxx True True\n",
      "not not ADV RB  xxx True True\n",
      "inﬂuence inﬂuence    xxxx True False\n",
      "how how    xxx True True\n",
      "features feature    xxxx True False\n",
      "are be    xxx True True\n",
      "deﬁned deﬁned    xxxx True False\n",
      "in in    xx True True\n",
      "anyway anyway    xxxx True True\n",
      ". .    . False False\n",
      "If If    Xx True False\n",
      "logistic logistic    xxxx True False\n",
      "regression regression    xxxx True False\n",
      "were be    xxxx True True\n",
      "given give    xxxx True False\n",
      "an a    xx True True\n",
      "MRI MRI    XXX True False\n",
      "scan scan    xxxx True False\n",
      "of of    xx True True\n",
      "the the    xxx True True\n",
      "patient patient    xxxx True False\n",
      ", ,    , False False\n",
      "rather rather    xxxx True True\n",
      "thanthe thanthe    xxxx True False\n",
      "doctor doctor    xxxx True False\n",
      "’s ’s    ’x False False\n",
      "formalized formalize    xxxx True False\n",
      "report report    xxxx True False\n",
      ", ,    , False False\n",
      "it it    xx True True\n",
      "would would    xxxx True True\n",
      "not not    xxx True True\n",
      "be be    xx True True\n",
      "able able    xxxx True False\n",
      "to to    xx True True\n",
      "make make    xxxx True True\n",
      "useful useful    xxxx True False\n",
      "predictions prediction    xxxx True False\n",
      ". .    . False False\n",
      "Individual Individual    Xxxxx True False\n",
      "pixels pixel    xxxx True False\n",
      "in in    xx True True\n",
      "an a    xx True True\n",
      "MRI MRI    XXX True False\n",
      "scan scan    xxxx True False\n",
      "have have    xxxx True True\n",
      "negligible negligible    xxxx True False\n",
      "correlation correlation    xxxx True False\n",
      "with with    xxxx True True\n",
      "any any    xxx True True\n",
      "complications complication    xxxx True False\n",
      "that that    xxxx True True\n",
      "might may    xxxx True True\n",
      "occur occur    xxxx True False\n",
      "during during    xxxx True True\n",
      "delivery delivery    xxxx True False\n",
      ". .    . False False\n",
      "This This    Xxxx True False\n",
      "dependence dependence    xxxx True False\n",
      "on on    xx True True\n",
      "representations representation    xxxx True False\n",
      "is be    xx True True\n",
      "a a    x True True\n",
      "general general    xxxx True False\n",
      "phenomenon phenomenon    xxxx True False\n",
      "that that    xxxx True True\n",
      "appears appear    xxxx True False\n",
      "throughout throughout    xxxx True True\n",
      "computer computer    xxxx True False\n",
      "science science    xxxx True False\n",
      "and and    xxx True True\n",
      "even even    xxxx True True\n",
      "daily daily    xxxx True False\n",
      "life life    xxxx True False\n",
      ". .    . False False\n",
      "In In    Xx True False\n",
      "computer computer    xxxx True False\n",
      "science science    xxxx True False\n",
      ", ,    , False False\n",
      "operations operation    xxxx True False\n",
      "such such    xxxx True True\n",
      "as a    xx True True\n",
      "searching search    xxxx True False\n",
      "a a    x True True\n",
      "collection collection    xxxx True False\n",
      "of of    xx True True\n",
      "data datum    xxxx True False\n",
      "can can    xxx True True\n",
      "proceed proceed    xxxx True False\n",
      "exponentially exponentially    xxxx True False\n",
      "faster fast    xxxx True False\n",
      "if if    xx True True\n",
      "the the    xxx True True\n",
      "collection collection    xxxx True False\n",
      "is be    xx True True\n",
      "structured structure    xxxx True False\n",
      "and and    xxx True True\n",
      "indexed index    xxxx True False\n",
      "intelligently intelligently    xxxx True False\n",
      ". .    . False False\n",
      "People People    Xxxxx True False\n",
      "can can    xxx True True\n",
      "easily easily    xxxx True False\n",
      "perform perform    xxxx True False\n",
      "arithmetic arithmetic    xxxx True False\n",
      "on on    xx True True\n",
      "Arabic Arabic    Xxxxx True False\n",
      "numerals numeral    xxxx True False\n",
      "but but    xxx True True\n",
      "ﬁnd ﬁnd    xxx True False\n",
      "arithmetic arithmetic    xxxx True False\n",
      "on on    xx True True\n",
      "Roman Roman    Xxxxx True False\n",
      "numerals numeral    xxxx True False\n",
      "much much    xxxx True True\n",
      "more much    xxxx True True\n",
      "time time    xxxx True False\n",
      "consuming consume    xxxx True False\n",
      ". .    . False False\n",
      "It It    Xx True False\n",
      "is be    xx True True\n",
      "not not    xxx True True\n",
      "surprising surprise    xxxx True False\n",
      "that that    xxxx True True\n",
      "the the    xxx True True\n",
      "choice choice    xxxx True False\n",
      "of of    xx True True\n",
      "representation representation    xxxx True False\n",
      "has have    xxx True True\n",
      "an a    xx True True\n",
      "enormous enormous    xxxx True False\n",
      "eﬀect eﬀect    xxxx True False\n",
      "on on    xx True True\n",
      "the the    xxx True True\n",
      "performance performance    xxxx True False\n",
      "of of    xx True True\n",
      "machine machine    xxxx True False\n",
      "learning learn    xxxx True False\n",
      "algorithms algorithm    xxxx True False\n",
      ". .    . False False\n",
      "For For    Xxx True False\n",
      "a a    x True True\n",
      "simple simple    xxxx True False\n",
      "visual visual    xxxx True False\n",
      "example example    xxxx True False\n",
      ", ,    , False False\n",
      "see see    xxx True True\n",
      "ﬁgure ﬁgure    xxxx True False\n",
      "1.1 1.1    d.d False False\n",
      ". .    . False False\n",
      "Many Many    Xxxx True False\n",
      " True\n",
      "providing provide    xxxx True False\n",
      "these this    xxxx True True\n",
      "features feature    xxxx True False\n",
      "to to    xx True True\n",
      "a a    x True True\n",
      "simple simple    xxxx True False\n",
      "machine machine    xxxx True False\n",
      "learning learn    xxxx True False\n",
      "algorithm algorithm    xxxx True False\n",
      ". .    . False False\n",
      "For For    Xxx True False\n",
      "example example    xxxx True False\n",
      ", ,    , False False\n",
      "a a    x True True\n",
      "useful useful    xxxx True False\n",
      "feature feature    xxxx True False\n",
      "for for    xxx True True\n",
      "speaker speaker    xxxx True False\n",
      "identiﬁcation identiﬁcation    xxxx True False\n",
      "from from    xxxx True True\n",
      "sound sound    xxxx True False\n",
      "is be    xx True True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an a    xx True True\n",
      "estimate estimate    xxxx True False\n",
      "of of    xx True True\n",
      "the the    xxx True True\n",
      "size size    xxxx True False\n",
      "of of    xx True True\n",
      "the the    xxx True True\n",
      "speaker speaker    xxxx True False\n",
      "’s ’s    ’x False False\n",
      "vocal vocal    xxxx True False\n",
      "tract tract    xxxx True False\n",
      ". .    . False False\n",
      "This This    Xxxx True False\n",
      "feature feature    xxxx True False\n",
      "gives give    xxxx True False\n",
      "astrong astrong    xxxx True False\n",
      "clue clue"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "          token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 0 5 ORG\n",
      "Colombo 27 34 ORG\n",
      "$1 billion 53 63 MONEY\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Apple is looking at buying Colombo based startup for $1 billion')\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I just bought \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " shares of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    9 a.m.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " because the stock went up \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    30%\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    just 2 days\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " according to the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    WSJ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "doc = nlp('I just bought 2 shares of Apple at 9 a.m. because the stock went up 30% in just 2 days according to the WSJ')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">I am \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Lahiru\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and I just bought \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2 Apples\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " at \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    9 a.m.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " from \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    the Apple Inc.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " before the current stock went up by \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    1 billion $\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('I am Lahiru and I just bought 2 Apples at 9 a.m. from the Apple Inc. before the current stock went up by 1 billion $')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are apples.\n",
      "These are oranges.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"These are apples. These are oranges.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    " \n",
    "#doc = nlp('Wall Street Journal just published a piece on crypto currencies')\n",
    "#displacy.render(doc, style='dep', jupyter=True, options={'distance': 80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Journal NP Journal\n",
      "an interesting piece NP piece\n",
      "crypto currencies NP currencies\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.label_, chunk.root.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3197928453018144401\n",
      "coffee\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'I love coffee')\n",
    "print(doc.vocab.strings[u'coffee'])  # 3197928453018144401\n",
    "print(doc.vocab.strings[3197928453018144401])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I 4690420944186131903 X I I True False True en\n",
      "love 3702023516439754181 xxxx l ove True False False en\n",
      "coffee 3197928453018144401 xxxx c fee True False False en\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('I love coffee')\n",
    "for word in doc:\n",
    "    lexeme = doc.vocab[word.text]\n",
    "    print(lexeme.text, lexeme.orth, lexeme.shape_, lexeme.prefix_, lexeme.suffix_,\n",
    "          lexeme.is_alpha, lexeme.is_digit, lexeme.is_title, lexeme.lang_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7927212922616225\n",
      "0.7982259384182496\n",
      "0.8015801058229597\n"
     ]
    }
   ],
   "source": [
    "target = nlp(\"Cats are beautiful animals.\")\n",
    " \n",
    "doc1 = nlp(\"Dogs are awesome.\")\n",
    "doc2 = nlp(\"Some gorgeous creatures are felines.\")\n",
    "doc3 = nlp(\"Dolphins are swimming mammals.\")\n",
    " \n",
    "print(target.similarity(doc1))  # 0.8901765218466683\n",
    "print(target.similarity(doc2))  # 0.9115828449161616\n",
    "print(target.similarity(doc3))  # 0.782295675287610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars cars nsubj shift\n",
      "insurance liability liability dobj shift\n",
      "manufacturers manufacturers pobj toward\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Autonomous cars shift insurance liability toward manufacturers\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "          chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
